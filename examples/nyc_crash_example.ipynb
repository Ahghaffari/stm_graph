{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a897e2c",
   "metadata": {},
   "source": [
    "# NYC Traffic Crash prediction with STM-Graph\n",
    "\n",
    "This notebook demonstrates how to use the STM-Graph library to analyze New York City traffic crash data and do prediction. We'll go through the complete workflow:\n",
    "\n",
    "1. Loading and preprocessing the raw data\n",
    "2. Creating spatial mappings using Grid-based partitioning\n",
    "3. Extracting OpenStreetMap features / Urban Features Graph Creation\n",
    "4. Building a graph representation of the data\n",
    "5. Creating temporal graph dataset\n",
    "6. Visualizing spatial and temporal patterns\n",
    "7. Training a GNN model for crash prediction\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stm_graph_path = \"/home/ubuntu/STM-Graph/src\"\n",
    "\n",
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append(stm_graph_path)\n",
    "import stm_graph\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "# Define the data and output directories\n",
    "DATA_DIR = \"/mnt/data/nyc_crash_311\"\n",
    "DATASET = \"Motor_Vehicle_Collisions_-_Crashes_20241203.csv\"\n",
    "OUTPUT_DIR = \"/mnt/data/nyc_crash_311/stm_graph/nyc/crash\"\n",
    "\n",
    "# Define geographic boundaries for NYC\n",
    "NYC_BOUNDS = {\n",
    "    \"min_lat\": 40.4774,  # Southern boundary\n",
    "    \"max_lat\": 40.9176,  # Northern boundary\n",
    "    \"min_lon\": -74.2591,  # Western boundary\n",
    "    \"max_lon\": -73.7004,  # Eastern boundary\n",
    "}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07fc3d",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Preprocessing\n",
    "\n",
    "First, we'll load the NYC crash data from a CSV file and perform some initial preprocessing. We'll convert the separate date and time columns into a single datetime column for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the raw data\n",
    "print(f\"Reading raw crash data from {os.path.join(DATA_DIR, DATASET)}\")\n",
    "raw_df = pd.read_csv(os.path.join(DATA_DIR, DATASET), low_memory=False)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {raw_df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(raw_df.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0272dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Combine date and time to create a datetime column\n",
    "print(\"Combining date and time columns...\")\n",
    "raw_df[\"created_time\"] = pd.to_datetime(\n",
    "    raw_df[\"CRASH DATE\"] + \" \" + raw_df[\"CRASH TIME\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Create a temporary CSV file with the combined datetime column\n",
    "temp_csv = os.path.join(DATA_DIR, \"temp_combined_crash.csv\")\n",
    "raw_df.to_csv(temp_csv, index=False)\n",
    "\n",
    "print(f\"Time range of data: {raw_df['created_time'].min()} to {raw_df['created_time'].max()}\")\n",
    "raw_df[[\"CRASH DATE\", \"CRASH TIME\", \"created_time\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aeeb98",
   "metadata": {},
   "source": [
    "## STM-Graph Preprocessing\n",
    "\n",
    "Now we'll use STM-Graph's preprocessing functionality to clean the data, convert it to a GeoDataFrame, and filter it to a specific time range. This step handles missing coordinates, converts the data to a spatial format, and prepares it for mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95001ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with STM-Graph\n",
    "print(\"Processing with STM-Graph...\")\n",
    "gdf_crash = stm_graph.preprocess_dataset(\n",
    "    data_path=DATA_DIR,\n",
    "    dataset=\"temp_combined_crash.csv\",\n",
    "    time_col=\"created_time\",\n",
    "    lat_col=\"LATITUDE\",\n",
    "    lng_col=\"LONGITUDE\",\n",
    "    column_mapping={\n",
    "        \"created_time\": \"created_time\",\n",
    "        \"BOROUGH\": \"borough\",\n",
    "        \"ZIP CODE\": \"zip_code\",\n",
    "        \"LATITUDE\": \"latitude\",\n",
    "        \"LONGITUDE\": \"longitude\",\n",
    "        \"LOCATION\": \"location\",\n",
    "        \"COLLISION_ID\": \"collision_id\",\n",
    "        \"NUMBER OF PERSONS INJURED\": \"injured_count\",\n",
    "        \"NUMBER OF PERSONS KILLED\": \"killed_count\",\n",
    "    },\n",
    "    filter_dates=(None, \"2019-12-31 23:59:59\"),\n",
    "    testing_mode=True,\n",
    "    test_bounds=NYC_BOUNDS,\n",
    "    visualize=True,\n",
    "    fig_format=\"png\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    show_background_map=True,\n",
    ")\n",
    "\n",
    "print(f\"Processed dataset shape: {gdf_crash.shape}\")\n",
    "print(f\"Time range: {gdf_crash['created_time'].min()} to {gdf_crash['created_time'].max()}\")\n",
    "gdf_crash.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56214db3",
   "metadata": {},
   "source": [
    "## 2. Spatial Mapping\n",
    "\n",
    "In this step, we'll apply a spatial mapping to divide NYC into grid cells. STM-Graph supports various mapping approaches and will be extended in later releases:\n",
    "\n",
    "1. Administrative boundaries (such as community districts, census tracts)\n",
    "2. Regular grid cells\n",
    "3. Degree-based Voronoi partitioning\n",
    "\n",
    "For this example, we'll use a Grid-based mapping with a cell size of 1 kilometer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7972f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grid mapping to the data\n",
    "print(\"Applying spatial mapping...\")\n",
    "\n",
    "# Create a grid mapping with 1000-meter cells\n",
    "mapper = stm_graph.GridMapping(cell_size=1000.0, target_crs=\"EPSG:32618\")\n",
    "\n",
    "# Apply the mapping to get district geometries and point-to-partition mapping\n",
    "district_gdf, point_to_partition = mapper.create_mapping(gdf_crash)\n",
    "\n",
    "print(f\"Created mapping with {len(district_gdf)} regions\")\n",
    "print(f\"Points with valid mapping: {(point_to_partition >= 0).sum()} of {len(point_to_partition)}\")\n",
    "\n",
    "# Visualize the mapping\n",
    "mapper.visualize(\n",
    "    points_gdf=gdf_crash,\n",
    "    partition_gdf=district_gdf,\n",
    "    point_to_partition=point_to_partition,\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    file_format=\"png\"\n",
    ")\n",
    "\n",
    "district_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c861e",
   "metadata": {},
   "source": [
    "## 3. OSM Feature Extraction / Urban Features Graph Creation\n",
    "\n",
    "Next, we'll extract features from OpenStreetMap (OSM) to enrich our model with contextual information about each area. Features can be used as raw normalized features by area or they can be used after embedding. These features include information about:\n",
    "\n",
    "- Points of interest (POIs)\n",
    "- Road networks\n",
    "- Road junctions\n",
    "\n",
    "These features provide important context about the urban environment that may influence event patterns. These features can get enhanced with later releases to add more features to select by user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature types to extract\n",
    "feature_types = ['poi', 'road', 'junction']\n",
    "\n",
    "# Extract OSM features\n",
    "osm_cache_dir = os.path.join(OUTPUT_DIR, \"osm_cache\")\n",
    "osm_features = stm_graph.extract_osm_features(\n",
    "    regions_gdf=district_gdf,\n",
    "    bounds=NYC_BOUNDS,\n",
    "    cache_dir=osm_cache_dir,\n",
    "    feature_types=feature_types,\n",
    "    normalize=True,\n",
    "    meter_crs=\"EPSG:32618\",\n",
    "    lat_lon_crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Print available features\n",
    "print(f\"Extracted {len(osm_features.columns)} OSM features\")\n",
    "print(\"\\nFeature sample:\")\n",
    "osm_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94aab5",
   "metadata": {},
   "source": [
    "## 4. Graph Construction\n",
    "\n",
    "Now we'll build a graph representation of our data. In this graph:\n",
    "- Nodes represent grid cells\n",
    "- Edges represent adjacency relationships between cells\n",
    "- Node features include OSM features and crash statistics\n",
    "\n",
    "This graph structure allows us to use Graph Neural Networks (GNNs) to model the spatial relationships between different areas of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd57963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter points that have valid mappings\n",
    "gdf_crash_valid = gdf_crash[point_to_partition >= 0].copy()\n",
    "point_to_partition_valid = point_to_partition[point_to_partition >= 0].copy()\n",
    "\n",
    "print(f\"Using {len(gdf_crash_valid)} valid points for graph construction\")\n",
    "\n",
    "# Build graph with static features\n",
    "graph_data = stm_graph.build_graph_and_augment(\n",
    "    grid_gdf=district_gdf,\n",
    "    points_gdf=gdf_crash_valid,\n",
    "    point_to_cell=point_to_partition_valid,\n",
    "    adj_matrix=None,\n",
    "    remove_empty_nodes=True,\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    save_flag=True,\n",
    "    static_features=osm_features,\n",
    ")\n",
    "\n",
    "# Extract graph components\n",
    "edge_index = graph_data[\"edge_index\"]\n",
    "edge_weight = graph_data[\"edge_weight\"]\n",
    "node_features = graph_data[\"node_features\"]\n",
    "augmented_df = graph_data[\"augmented_df\"]\n",
    "node_ids = graph_data[\"node_ids\"]\n",
    "\n",
    "print(f\"Built graph with {edge_index.shape[1]} edges and {graph_data['num_nodes']} nodes\")\n",
    "print(f\"Node features shape: {node_features.shape}\")\n",
    "\n",
    "# Display augmented dataframe\n",
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e30b1",
   "metadata": {},
   "source": [
    "## 5. Temporal Dataset Creation\n",
    "\n",
    "With our graph structure in place, we'll now create a temporal dataset for time-aware analysis and prediction. We'll:\n",
    "\n",
    "1. Bin crashes into daily intervals\n",
    "2. Create sliding windows of data for training\n",
    "3. Add time-based features (day of week, hour of day)\n",
    "4. Normalize the features for better model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal dataset\n",
    "temporal_dataset, dataset_path, metadata = stm_graph.create_temporal_dataset(\n",
    "    edge_index=edge_index,\n",
    "    augmented_df=augmented_df,\n",
    "    edge_weights=edge_weight,\n",
    "    node_ids=node_ids,\n",
    "    static_features=osm_features,\n",
    "    time_col=\"created_time\",\n",
    "    cell_col=\"cell_id\",\n",
    "    bin_type=\"daily\",\n",
    "    interval_hours=1,\n",
    "    history_window=3,\n",
    "    use_time_features=False,\n",
    "    task=\"classification\",\n",
    "    horizon=1,\n",
    "    downsample_factor=1,\n",
    "    normalize=True,\n",
    "    scaler_type=\"minmax\",\n",
    "    dataset_name=\"nyc_crash_dataset\",\n",
    "    output_format=\"4d\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be614e",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Now let's create some visualizations to better understand our data. We'll create:\n",
    "\n",
    "1. Time series plots showing crash trends over time\n",
    "2. Spatial network visualizations showing crash density across NYC\n",
    "3. Temporal heatmaps showing patterns across time and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temporal_dataset_3d = stm_graph.convert_4d_to_3d_dataset(\n",
    "    temporal_dataset, static_features_count=osm_features.shape[1])\n",
    "\n",
    "# Plot time series for the most active nodes\n",
    "stm_graph.plot_node_time_series(\n",
    "    temporal_dataset_3d,\n",
    "    num_nodes=5,  # Show 5 nodes\n",
    "    selection_method=\"highest_activity\",  # Select most active nodes\n",
    "    feature_idx=0,  # Event count feature\n",
    "    plot_type=\"2d\",  # 2D line plot\n",
    "    start_time=\"2019-11-01\",  # Start date for x-axis\n",
    "    time_delta=timedelta(hours=1),  # Hourly data\n",
    "    title=\"Crash Events Over Time (Most Active Nodes)\",\n",
    "    figsize=(15, 8),\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    filename=\"time_series_top\",\n",
    "    file_format=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a51bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3D visualization for most active nodes\n",
    "stm_graph.plot_node_time_series(\n",
    "    temporal_dataset_3d,\n",
    "    num_nodes=3,  # Show 3 nodes\n",
    "    selection_method=\"highest_activity\",  # Select most active nodes\n",
    "    feature_idx=0,  # Event count feature\n",
    "    plot_type=\"3d\",  # 3D surface plot\n",
    "    n_steps=168,  # First week (7 days * 24 hours)\n",
    "    title=\"Crashes 3D Visualization Over Time\",\n",
    "    figsize=(15, 10),\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    filename=\"time_series_3d\",\n",
    "    file_format=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e16c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract event counts for each region (node) at a specific time\n",
    "time_step = 24  # Example: events after 24 hours\n",
    "node_counts = np.array(\n",
    "    [\n",
    "        temporal_dataset_3d.features[time_step][node, 0].item()\n",
    "        for node in range(graph_data[\"num_nodes\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot spatial network with region and edge colors\n",
    "stm_graph.plot_spatial_network(\n",
    "    regions_gdf=district_gdf,\n",
    "    edge_index=edge_index,\n",
    "    edge_weights=edge_weight,\n",
    "    node_values=node_counts,\n",
    "    node_ids=node_ids,\n",
    "    time_step=time_step,\n",
    "    title=\"Crash Density After 24 Hours\",\n",
    "    node_cmap=\"YlOrRd\",  # Red-yellow colormap for heat\n",
    "    edge_cmap=\"viridis\",  # Blue-green for edges\n",
    "    map_style=ctx.providers.CartoDB.Positron,\n",
    "    figsize=(15, 15),\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    filename=\"spatial_network\",\n",
    "    file_format=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporal heatmap to see patterns across time and nodes\n",
    "stm_graph.plot_temporal_heatmap(\n",
    "    temporal_dataset_3d,\n",
    "    num_nodes=10,\n",
    "    feature_idx=0,  # Event count feature\n",
    "    selection_method=\"highest_activity\",\n",
    "    start_time=\"2019-11-01\",\n",
    "    time_delta=timedelta(hours=1),\n",
    "    n_steps=168,  # First week\n",
    "    title=\"Crash Events Temporal Heatmap (First Week)\",\n",
    "    figsize=(15, 8),\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    filename=\"temporal_heatmap\",\n",
    "    file_format=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91867544",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "Finally, we'll train a Graph Neural Network (GNN) model to predict crash events. We'll use the DCRNN model.\n",
    "\n",
    "This model will Predict whether crashes will occur in each area in the next time step. Different designed custom models can be used or any supported model from Torch Geometric Temporal can be used. More custom models can be added. Training logs will be saved in logs folder locally in output directory. [Weights & Biases](https://wandb.ai/) integration is done and you can login and use online dashboard to control the training process and track training metrics and process live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available model options\n",
    "print(\"Available models in STM-Graph:\")\n",
    "stm_graph.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e12e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STGCN\n",
    "model = stm_graph.create_model(\n",
    "    model_name=\"stgcn\",\n",
    "    source=\"custom\",\n",
    "    num_nodes=temporal_dataset.features[0].shape[0],\n",
    "    in_channels=temporal_dataset.features[0].shape[2],\n",
    "    out_channels=1,\n",
    "    hidden_dim=64,\n",
    "    k=3,\n",
    "    embedding_dimensions=16,\n",
    "    dropout=0.2,\n",
    "    task=\"classification\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "results = stm_graph.train_model(\n",
    "    model=model,\n",
    "    dataset=temporal_dataset,\n",
    "    optimizer_name=\"adam\",\n",
    "    learning_rate=0.0001,\n",
    "    task=\"classification\",\n",
    "    num_epochs=500,  \n",
    "    batch_size=10,\n",
    "    batch_to_device=True,\n",
    "    test_size=0.15,\n",
    "    val_size=0.15,\n",
    "    use_nested_tqdm=True,\n",
    "    early_stopping=True,\n",
    "    patience=50,\n",
    "    scheduler_type=\"step\",\n",
    "    lr_decay_epochs=50,\n",
    "    lr_decay_factor=1,\n",
    "    wandb_project=\"stm_graph_crash\",\n",
    "    experiment_name=\"stgcn\",\n",
    "    use_wandb=True, \n",
    "    fixed_batch_size=True,\n",
    "    log_dir=OUTPUT_DIR,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
